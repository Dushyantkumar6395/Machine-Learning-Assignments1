{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec620c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "# Gradient Boosting Regression is a machine learning technique that builds an\n",
    "# ensemble of weak prediction models, typically decision trees, in a sequential manner. It aims to minimize the\n",
    "# error by optimizing a differentiable loss function, such as mean squared error for regression tasks. \n",
    "\n",
    "# In Gradient Boosting Regression, each new model in the ensemble corrects errors made by the previous models. \n",
    "# This is achieved by fitting the new model to the residual errors of the ensemble's prediction up to that point. \n",
    "# The final prediction is the sum of predictions from all models, weighted by a small learning rate to control the\n",
    "# contribution of each model. This approach results in a powerful regression model capable of capturing complex nonlinear \n",
    "# relationships in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b3e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "# simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "# performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.residuals = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize residuals with the mean of y\n",
    "        self.residuals.append(np.mean(y))\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            # Fit a new base learner (decision tree) to the residuals\n",
    "            model_t = DecisionTreeRegressor(max_depth=1)  # Decision stump\n",
    "            model_t.fit(X, self.residuals[-1])\n",
    "            self.models.append(model_t)\n",
    "\n",
    "            # Predict using the current ensemble\n",
    "            y_pred_t = self.predict(X)\n",
    "\n",
    "            # Compute residuals (negative gradient of the loss function)\n",
    "            residuals_t = y - y_pred_t\n",
    "            self.residuals.append(residuals_t)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Example usage:\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.normal(scale=1, size=100)\n",
    "\n",
    "# Initialize and train the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdc1deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'learning_rate': 0.05, 'max_depth': 1, 'n_estimators': 200}\n",
      "Best Score (MSE): 1.107534661996978\n",
      "Mean Squared Error (MSE): 0.42524657746438466\n",
      "R-squared score: 0.9875875805575595\n"
     ]
    }
   ],
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "# optimise the performance of the model. Use grid search or random search to find the best\n",
    "# hyperparameters\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.normal(scale=1, size=100)\n",
    "\n",
    "# Define the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation (5-fold CV)\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (MSE):\", -grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_gb_regressor = grid_search.best_estimator_\n",
    "y_pred = best_gb_regressor.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63568d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "# In the context of Gradient Boosting, a weak learner refers to a base model that performs slightly better than random guessing on a given classification or regression problem. The term \"weak\" does not imply that the model is inherently poor; rather, it suggests that the model's performance is just above chance level.\n",
    "\n",
    "# ### Characteristics of a Weak Learner:\n",
    "\n",
    "# 1. **Limited Complexity**: Weak learners are typically simple models with constrained complexity. For example, in Gradient Boosting for regression, weak learners are often decision trees with shallow depth (e.g., depth 1 or 2), also known as decision stumps. In classification tasks, weak learners could be decision trees or even linear models.\n",
    "\n",
    "# 2. **Slightly Better Than Chance**: A weak learner achieves accuracy or predictive power that is marginally better than random guessing. In practical terms, this means the model's performance is just enough to provide useful directional information on the problem.\n",
    "\n",
    "# 3. **Training Iteratively**: In Gradient Boosting, each weak learner is trained sequentially to improve upon the predictions made by the ensemble up to that point. Each new weak learner focuses on correcting the errors or residuals of the ensemble of all previous learners.\n",
    "\n",
    "# ### Role in Gradient Boosting:\n",
    "\n",
    "# - **Sequential Improvement**: By iteratively fitting weak learners to the residuals of the ensemble, Gradient Boosting builds a strong learner by combining the predictions of these sequentially trained models.\n",
    "  \n",
    "# - **Ensemble Learning**: The strength of Gradient Boosting lies in its ability to harness the collective wisdom of multiple weak learners. Each weak learner contributes a small piece to the overall prediction, and their combined effect improves the model's performance significantly.\n",
    "\n",
    "# ### Example:\n",
    "\n",
    "# In a regression scenario, a weak learner might be a decision stump that splits the data based on a single feature and predicts a constant value for each resulting segment. This decision stump alone may not provide accurate predictions, but when combined with other decision stumps trained on the residuals of the ensemble, it contributes to reducing the overall error.\n",
    "\n",
    "# ### Advantages:\n",
    "\n",
    "# - **Computational Efficiency**: Weak learners are often computationally inexpensive to train, making Gradient Boosting feasible for large datasets.\n",
    "  \n",
    "# - **Avoid Overfitting**: Because weak learners are simple and constrained, they are less prone to overfitting the training data, especially when combined in an ensemble.\n",
    "\n",
    "# ### Conclusion:\n",
    "\n",
    "# In Gradient Boosting, the concept of weak learners emphasizes the collaborative nature of the ensemble method, where multiple modestly performing models come together to achieve superior predictive accuracy. By focusing on correcting errors made by previous learners, Gradient Boosting effectively builds a robust and accurate predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64160706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "# The intuition behind the Gradient Boosting algorithm lies in iteratively improving upon the predictions of a sequence of weak learners, typically decision trees, to build a strong predictive model:\n",
    "\n",
    "# 1. **Sequential Correction**: Gradient Boosting starts with an initial weak learner (e.g., decision stump) and sequentially adds new learners to correct the errors or residuals of the ensemble up to that point.\n",
    "\n",
    "# 2. **Gradient Descent**: It uses gradient descent optimization to minimize a loss function (e.g., mean squared error for regression) by fitting each new learner to the negative gradient of the loss function with respect to the ensemble predictions.\n",
    "\n",
    "# 3. **Gradient as Guidance**: Each subsequent learner is trained to capture the remaining error left by the ensemble of all previous learners. This is akin to walking downhill in a gradient landscape towards a minimum error.\n",
    "\n",
    "# 4. **Learning Rate Control**: A learning rate parameter controls the contribution of each new learner to the ensemble, preventing overfitting by scaling down the impact of each learner.\n",
    "\n",
    "# 5. **Ensemble of Weak Predictors**: By combining the predictions of these sequentially trained weak learners, Gradient Boosting constructs a robust model capable of capturing complex relationships in data.\n",
    "\n",
    "# 6. **Reduction of Bias and Variance**: Through the iterative process of adding weak learners, Gradient Boosting reduces bias by fitting the data more closely and manages variance by averaging out the errors across multiple models.\n",
    "\n",
    "# 7. **Focus on Difficult Cases**: The algorithm focuses on improving predictions on instances that are challenging to classify or predict correctly, making it effective for handling skewed datasets or complex patterns.\n",
    "\n",
    "# 8. **Flexibility with Loss Functions**: Gradient Boosting can accommodate various loss functions, allowing it to be applied to both regression and classification tasks with appropriate adaptations.\n",
    "\n",
    "# 9. **Wide Applicability**: It is widely used in both academia and industry due to its effectiveness, adaptability, and ability to produce state-of-the-art results in various machine learning competitions and real-world applications.\n",
    "\n",
    "# 10. **Intuitive Evolution**: Overall, the intuition behind Gradient Boosting revolves around iteratively refining predictions by learning from mistakes, leveraging weak models to form a robust and accurate ensemble predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6606e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "# The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner to create a strong predictive model. Here’s how it constructs this ensemble:\n",
    "\n",
    "# 1. **Initialization**:\n",
    "#    - Starts with an initial weak learner, often a simple model like a decision stump (tree with one node), which provides a baseline prediction.\n",
    "\n",
    "# 2. **Fitting the First Learner**:\n",
    "#    - Trains the first weak learner on the training data to predict the target variable. This initial model captures the broad patterns in the data but typically has high bias and may underfit.\n",
    "\n",
    "# 3. **Sequential Improvement**:\n",
    "#    - Iteratively improves upon the predictions of the ensemble by adding new weak learners. Each subsequent learner focuses on correcting the errors (residuals) made by the ensemble up to that point.\n",
    "\n",
    "# 4. **Gradient Descent Optimization**:\n",
    "#    - Utilizes gradient descent optimization to minimize a predefined loss function (e.g., mean squared error for regression, log loss for classification). It fits each new learner to the negative gradient of the loss function with respect to the ensemble predictions.\n",
    "\n",
    "# 5. **Weighting and Learning Rate**:\n",
    "#    - Introduces a learning rate parameter that scales the contribution of each new learner to the ensemble. This parameter controls the step size in the direction of minimizing the loss function, helping to prevent overfitting.\n",
    "\n",
    "# 6. **Residuals and Gradient Calculation**:\n",
    "#    - At each iteration, calculates the residuals (errors) between the current predictions of the ensemble and the true target values. The new learner then focuses on predicting these residuals more accurately.\n",
    "\n",
    "# 7. **Combining Predictions**:\n",
    "#    - Combines predictions from all weak learners using a weighted sum. Each model’s weight reflects its contribution to minimizing the overall loss function.\n",
    "\n",
    "# 8. **Reduction of Bias and Variance**:\n",
    "#    - Gradually reduces bias by fitting more complex models to the residuals, capturing finer details and reducing errors in predictions.\n",
    "#    - Manages variance by averaging out the errors across multiple models, improving the robustness and generalization of the ensemble.\n",
    "\n",
    "# 9. **Stopping Criteria**:\n",
    "#    - Stops adding new learners based on predefined stopping criteria, such as reaching a maximum number of iterations or when further additions do not significantly improve performance.\n",
    "\n",
    "# 10. **Final Ensemble Model**:\n",
    "#     - The final prediction of the Gradient Boosting ensemble is a weighted sum of predictions from all weak learners, scaled by the learning rate. This ensemble approach results in a powerful and accurate model capable of handling complex relationships in data.\n",
    "\n",
    "# In essence, Gradient Boosting builds an ensemble of weak learners by iteratively fitting each new learner to the residuals of the ensemble’s predictions, progressively improving model performance and creating a robust predictive model suitable for a wide range of machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8d4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "# algorithm?\n",
    "# Constructing the mathematical intuition behind the Gradient Boosting algorithm involves several key steps that outline how the algorithm iteratively builds an ensemble of weak learners to optimize a loss function. Here are the fundamental steps involved:\n",
    "\n",
    "# 1. **Initialization**:\n",
    "#    - Start with an initial prediction \\( F_0(x) \\), often set to a simple value such as the mean of the target variable for regression or the logarithm of class probabilities for classification.\n",
    "\n",
    "# 2. **Compute Pseudo-Residuals**:\n",
    "#    - For each iteration \\( m \\):\n",
    "#      - Compute the negative gradient (pseudo-residuals) of the loss function with respect to the current ensemble predictions:\n",
    "#        \\[\n",
    "#        r_{im} = -\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\Bigg|_{F(x_i) = F_{m-1}(x_i)}\n",
    "#        \\]\n",
    "#      where \\( L \\) is the loss function (e.g., mean squared error for regression, log loss for classification), \\( y_i \\) is the true target value, and \\( F_{m-1}(x_i) \\) is the ensemble prediction up to iteration \\( m-1 \\).\n",
    "\n",
    "# 3. **Fit a Weak Learner**:\n",
    "#    - Train a weak learner \\( h_m(x) \\) (e.g., decision tree) to predict the pseudo-residuals \\( r_{im} \\). The weak learner is typically constrained to be a simple model to prevent overfitting (e.g., shallow decision tree stumps).\n",
    "\n",
    "# 4. **Update Ensemble Prediction**:\n",
    "#    - Update the ensemble prediction by adding a scaled version of the new weak learner:\n",
    "#      \\[\n",
    "#      F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)\n",
    "#      \\]\n",
    "#      where \\( \\nu \\) (learning rate) is a parameter that scales the contribution of each weak learner \\( h_m(x) \\) to the ensemble.\n",
    "\n",
    "# 5. **Iterate**:\n",
    "#    - Repeat steps 2-4 for \\( M \\) iterations or until a stopping criterion is met (e.g., no significant improvement in the loss function).\n",
    "\n",
    "# 6. **Final Prediction**:\n",
    "#    - The final prediction \\( F_M(x) \\) is the cumulative sum of predictions from all weak learners:\n",
    "#      \\[\n",
    "#      F_M(x) = F_0(x) + \\nu \\sum_{m=1}^{M} h_m(x)\n",
    "#      \\]\n",
    "\n",
    "# 7. **Loss Function Optimization**:\n",
    "#    - Gradient Boosting aims to minimize the loss function \\( L(y, F(x)) \\) over the training dataset by iteratively fitting weak learners to the negative gradients of the loss function.\n",
    "\n",
    "# ### Mathematical Intuition:\n",
    "\n",
    "# - **Gradient Descent**: Each step in Gradient Boosting resembles a gradient descent step in function space, where the negative gradient guides the creation of subsequent weak learners.\n",
    "  \n",
    "# - **Sequential Learning**: The algorithm sequentially corrects errors made by the ensemble of weak learners by fitting new models to the residuals of the current ensemble predictions.\n",
    "  \n",
    "# - **Ensemble Construction**: By combining weak learners, each focusing on different aspects of the prediction errors, Gradient Boosting constructs a strong ensemble model that captures complex relationships in the data.\n",
    "\n",
    "# Understanding these steps provides a clear mathematical framework for how Gradient Boosting optimizes a loss function and builds an ensemble of weak learners to achieve a powerful predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00bc89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
