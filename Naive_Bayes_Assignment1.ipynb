{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f97560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Bayes' theorem?\n",
    "# Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental theorem in probability theory that describes how to \n",
    "# update the probability of a hypothesis (an event or proposition) based on new evidence or information. \n",
    "\n",
    "# Mathematically, Bayes' theorem can be expressed as:\n",
    "\n",
    "# \\[ P(H | E) = \\frac{P(E | H) \\cdot P(H)}{P(E)} \\]\n",
    "\n",
    "# Where:\n",
    "# - \\( P(H | E) \\) is the posterior probability of hypothesis \\( H \\) given evidence \\( E \\).\n",
    "# - \\( P(E | H) \\) is the probability of observing evidence \\( E \\) given that \\( H \\) is true (likelihood).\n",
    "# - \\( P(H) \\) is the prior probability of hypothesis \\( H \\) being true before considering the evidence.\n",
    "# - \\( P(E) \\) is the total probability of observing the evidence \\( E \\) (also known as the marginal likelihood or evidence).\n",
    "\n",
    "# Bayes' theorem is foundational in statistics, machine learning, and various fields where inference and decision-making under\n",
    "# uncertainty are crucial. It allows us to revise our beliefs about the likelihood of hypotheses in light of new data or \n",
    "# observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6374bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the formula for Bayes' theorem?\n",
    "# P(H∣E)= P(E∣H)⋅P(H)/P(E)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96056401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How is Bayes' theorem used in practice?\n",
    "# Bayes' theorem is applied in practice to update beliefs or probabilities in light of new evidence.\n",
    "# It's used in various fields such as statistics, machine learning, and medical diagnosis. \n",
    "# For example, in medical diagnosis, it helps calculate the probability of a disease given certain symptoms, \n",
    "# incorporating both the initial likelihood of having the disease (prior probability) and the probability of \n",
    "# exhibiting those symptoms if the disease is present (likelihood). This updated probability (posterior probability) \n",
    "# guides decision-making, treatment plans, and risk assessments based on the latest available information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a47e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "# Bayes' theorem and conditional probability are closely related concepts in probability theory:\n",
    "\n",
    "# 1. **Bayes' Theorem**: Bayes' theorem provides a way to update the probability of a hypothesis \\( H \\) given evidence \\( E \\), using the conditional probabilities of \\( E \\) given \\( H \\) (\\( P(E | H) \\)), the prior probability of \\( H \\) (\\( P(H) \\)), and the total probability of \\( E \\) (\\( P(E) \\)).\n",
    "\n",
    "#    \\[ P(H | E) = \\frac{P(E | H) \\cdot P(H)}{P(E)} \\]\n",
    "\n",
    "# 2. **Conditional Probability**: Conditional probability refers to the probability of an event \\( E \\) occurring given that another event \\( H \\) has occurred, denoted as \\( P(E | H) \\).\n",
    "\n",
    "#    \\[ P(E | H) = \\frac{P(E \\cap H)}{P(H)} \\]\n",
    "\n",
    "# The relationship lies in how Bayes' theorem utilizes conditional probabilities to update the prior probability \\( P(H) \\) into the posterior probability \\( P(H | E) \\). It connects the initial belief (prior) with new evidence, adjusting the belief based on how likely the evidence is under different scenarios (likelihood). Thus, Bayes' theorem formalizes the process of updating beliefs using conditional probabilities, making it a fundamental tool in probabilistic reasoning and decision-making under uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a2b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "# Choosing the appropriate type of Naive Bayes classifier depends on the nature of the problem and the characteristics of the\n",
    "# data. Here are considerations for selecting the type of Naive Bayes classifier:\n",
    "\n",
    "# 1. **Gaussian Naive Bayes**: \n",
    "#    - **Nature of Features**: It assumes continuous features that follow a Gaussian (normal) distribution.\n",
    "#    - **Example**: Suitable for problems where features are real-valued and assumed to be normally distributed,\n",
    "#     such as in some natural language processing tasks or when dealing with sensor data.\n",
    "\n",
    "# 2. **Multinomial Naive Bayes**: \n",
    "#    - **Nature of Features**: It is used when features represent counts or frequencies of occurrences \n",
    "#     (e.g., word counts in text classification).\n",
    "#    - **Example**: Commonly used in text classification tasks where the frequency of words or terms is used as features.\n",
    "\n",
    "# 3. **Bernoulli Naive Bayes**: \n",
    "#    - **Nature of Features**: It assumes binary features (presence or absence of a feature).\n",
    "#    - **Example**: Useful for text classification tasks where each term occurrence is binary\n",
    "#     (e.g., presence of a word in a document).\n",
    "\n",
    "# To decide which type to use:\n",
    "\n",
    "# - **Data Representation**: Understand how features are represented in your dataset (continuous, counts, binary).\n",
    "# - **Assumptions**: Consider whether the assumptions of each Naive Bayes type (like Gaussian distribution for Gaussian NB) \n",
    "#     align with your data.\n",
    "# - **Performance**: Sometimes, testing multiple types and evaluating their performance via cross-validation can help determine \n",
    "#     which works best for your specific problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 - You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "# Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "# each feature value for each class:\n",
    "# Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "# A 3 3 4 4 3 3 3\n",
    "# B 2 2 1 2 2 2 3\n",
    "# Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "# to belong to?\n",
    "# To predict the class of a new instance with features \\( X1 = 3 \\) and \\( X2 = 4 \\) using Naive Bayes, we will calculate the posterior probabilities for each class \\( A \\) and \\( B \\) based on the given data and assuming equal prior probabilities (\\( P(A) = P(B) = 0.5 \\)).\n",
    "\n",
    "# Given data:\n",
    "# - Class A:\n",
    "#   - \\( P(X1=3 | A) = \\frac{4}{10} \\)\n",
    "#   - \\( P(X2=4 | A) = \\frac{3}{10} \\)\n",
    "# - Class B:\n",
    "#   - \\( P(X1=3 | B) = \\frac{1}{7} \\)\n",
    "#   - \\( P(X2=4 | B) = \\frac{3}{7} \\)\n",
    "\n",
    "# Since the Naive Bayes assumption states that features are conditionally independent given the class, we can compute the posterior probabilities \\( P(A | X1=3, X2=4) \\) and \\( P(B | X1=3, X2=4) \\) using Bayes' theorem:\n",
    "\n",
    "# \\[ P(A | X1=3, X2=4) \\propto P(X1=3 | A) \\cdot P(X2=4 | A) \\cdot P(A) \\]\n",
    "# \\[ P(B | X1=3, X2=4) \\propto P(X1=3 | B) \\cdot P(X2=4 | B) \\cdot P(B) \\]\n",
    "\n",
    "# Let's calculate:\n",
    "\n",
    "# For Class A:\n",
    "# \\[ P(X1=3 | A) \\cdot P(X2=4 | A) \\cdot P(A) = \\frac{4}{10} \\cdot \\frac{3}{10} \\cdot 0.5 = \\frac{12}{100} \\cdot 0.5 = 0.06 \\]\n",
    "\n",
    "# For Class B:\n",
    "# \\[ P(X1=3 | B) \\cdot P(X2=4 | B) \\cdot P(B) = \\frac{1}{7} \\cdot \\frac{3}{7} \\cdot 0.5 = \\frac{3}{49} \\cdot 0.5 = 0.0306 \\]\n",
    "\n",
    "# Normalize these probabilities (sum to 1):\n",
    "# \\[ P(A | X1=3, X2=4) = \\frac{0.06}{0.06 + 0.0306} = \\frac{0.06}{0.0906} \\approx 0.6623 \\]\n",
    "# \\[ P(B | X1=3, X2=4) = \\frac{0.0306}{0.06 + 0.0306} = \\frac{0.0306}{0.0906} \\approx 0.3377 \\]\n",
    "\n",
    "# Therefore, the Naive Bayes classifier would predict that the new instance with \\( X1 = 3 \\) and \\( X2 = 4 \\) belongs to **Class A** because \\( P(A | X1=3, X2=4) > P(B | X1=3, X2=4) \\)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
