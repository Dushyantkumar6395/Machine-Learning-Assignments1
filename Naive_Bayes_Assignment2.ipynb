{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04fe7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "# probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "# Answer:\n",
    "# the probability that an employee is a smoker given that he/she uses the health insurance plan is \n",
    "# 0.4, or 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba71efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "# Bernoulli Naive Bayes and Multinomial Naive Bayes are both variations of the Naive Bayes classifier, but they are suited to different types of data and have different assumptions about the data distribution. Here are the key differences:\n",
    "\n",
    "# ### Bernoulli Naive Bayes\n",
    "# 1. **Data Type**: Bernoulli Naive Bayes is used for binary/boolean features. It works well when the features are binary (0/1, True/False, Yes/No).\n",
    "# 2. **Model Assumption**: It assumes that features are binary variables and models the presence or absence of a feature.\n",
    "# 3. **Feature Representation**: Each feature is treated as a binary value indicating whether a particular feature is present or absent.\n",
    "# 4. **Application**: It is commonly used in text classification problems where the presence or absence of a word matters, such as spam detection.\n",
    "# 5. **Probability Calculation**: It calculates the probability using a Bernoulli distribution. For each feature, it estimates the probability that the feature is present (or absent) given the class.\n",
    "\n",
    "# ### Multinomial Naive Bayes\n",
    "# 1. **Data Type**: Multinomial Naive Bayes is used for discrete count data. It works well with features that represent counts or frequencies, such as the frequency of words in a document.\n",
    "# 2. **Model Assumption**: It assumes that the features are generated from a multinomial distribution.\n",
    "# 3. **Feature Representation**: Each feature represents the frequency or count of a term in a document.\n",
    "# 4. **Application**: It is commonly used in text classification problems where word frequency matters, such as document classification or sentiment analysis.\n",
    "# 5. **Probability Calculation**: It calculates the probability using a multinomial distribution. For each feature, it estimates the probability of observing a given count of the feature given the class.\n",
    "\n",
    "# ### Summary of Differences\n",
    "# - **Feature Type**: Bernoulli Naive Bayes is for binary features, while Multinomial Naive Bayes is for count/frequency features.\n",
    "# - **Use Case**: Bernoulli is typically used for problems involving binary presence/absence of features, whereas Multinomial is used for problems involving frequency/count of features.\n",
    "# - **Probability Distribution**: Bernoulli Naive Bayes uses the Bernoulli distribution for binary outcomes, while Multinomial Naive Bayes uses the multinomial distribution for count data.\n",
    "\n",
    "# In conclusion, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the feature data: use Bernoulli for binary data and Multinomial for count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "Bernoulli Naive Bayes, like other variations of the Naive Bayes classifier, typically assumes that the data is complete and does not inherently provide a mechanism for handling missing values. However, there are some common strategies to handle missing values before applying the Bernoulli Naive Bayes algorithm:\n",
    "\n",
    "### Common Strategies for Handling Missing Values\n",
    "\n",
    "1. **Imputation**:\n",
    "   - **Mean/Median/Mode Imputation**: For binary data, you can replace missing values with the mode (most frequent value) of the feature.\n",
    "   - **Domain-specific Imputation**: Use domain knowledge to replace missing values. For example, if a binary feature indicates the presence of a specific condition and its absence is rare, you might impute missing values with 0 (absence).\n",
    "\n",
    "2. **Indicator Variable**:\n",
    "   - Create a new binary feature indicating whether the original feature value was missing. This way, the model can learn if the presence of a missing value itself carries predictive information.\n",
    "\n",
    "3. **Model-based Imputation**:\n",
    "   - Use a predictive model to estimate the missing values. For example, you can train a simple logistic regression model on the non-missing data to predict the missing values.\n",
    "\n",
    "4. **Ignore Missing Values**:\n",
    "   - If the number of missing values is very small, you might choose to ignore instances with missing values during training and prediction.\n",
    "\n",
    "### Example: Imputation for Bernoulli Naive Bayes\n",
    "Let's say you have a dataset with a binary feature `X1` and some missing values:\n",
    "```\n",
    "X1 = [1, 0, 1, 1, ?, 0, 1, ?]\n",
    "```\n",
    "You might impute the missing values as follows:\n",
    "- Calculate the mode of `X1` (most frequent value). Here, the mode is 1.\n",
    "- Replace the missing values with 1:\n",
    "```\n",
    "X1_imputed = [1, 0, 1, 1, 1, 0, 1, 1]\n",
    "```\n",
    "\n",
    "### Handling Missing Values During Prediction\n",
    "When making predictions with Bernoulli Naive Bayes, if you encounter missing values in the input data, you can apply similar imputation techniques or use a previously trained imputation model to fill in the missing values before making the prediction.\n",
    "\n",
    "### Summary\n",
    "While Bernoulli Naive Bayes itself does not handle missing values directly, pre-processing steps such as imputation, creating indicator variables, or using model-based approaches can be employed to handle missing values before feeding the data into the Bernoulli Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "# Yes, Gaussian Naive Bayes can be used for multi-class classification. In fact, it is well-suited for this task.\n",
    "# Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution and applies Bayes'\n",
    "# theorem to predict the probability of each class given the feature values.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
