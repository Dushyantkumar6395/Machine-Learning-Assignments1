{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f9ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "# Polynomial functions and kernel functions in machine learning are closely related concepts, especially in the context of Support Vector Machines (SVMs) and other kernel-based methods:\n",
    "\n",
    "# 1. **Kernel Functions**: Kernel functions compute the inner product (similarity) between pairs of data points in a higher-dimensional space without explicitly transforming them. Common kernel functions include polynomial, Gaussian (RBF), and sigmoid kernels.\n",
    "\n",
    "# 2. **Polynomial Functions**: Polynomial functions are a specific type of kernel function that compute the dot product of vectors raised to a power, often used for capturing nonlinear relationships in data.\n",
    "\n",
    "# 3. **Nonlinear Mapping**: Both polynomial functions and kernel functions aim to capture nonlinear relationships by mapping data into higher-dimensional spaces where they might become linearly separable.\n",
    "\n",
    "# 4. **SVMs**: In SVMs, polynomial kernels (\\( K(\\mathbf{x}, \\mathbf{x'}) = (\\gamma \\mathbf{x}^\\top \\mathbf{x'} + r)^d \\)) explicitly compute polynomial transformations in feature space, allowing SVMs to model nonlinear decision boundaries.\n",
    "\n",
    "# 5. **Relationship**: Polynomial kernels are a specific implementation of kernel functions where data is implicitly transformed into a higher-dimensional space using polynomial transformations. Other kernel functions, like RBF kernels, use different transformations suited for specific tasks.\n",
    "\n",
    "# 6. **Flexibility**: Kernel functions, including polynomial kernels, provide flexibility in SVMs by enabling them to handle complex patterns in data without explicitly computing the transformations, which would be computationally expensive in high-dimensional spaces.\n",
    "\n",
    "# 7. **Parameterization**: Polynomial kernels are parameterized by parameters such as degree \\( d \\) and coefficient \\( \\gamma \\), influencing the complexity and flexibility of the decision boundary learned by SVMs.\n",
    "\n",
    "# 8. **Performance**: The choice between polynomial kernels and other kernels depends on the problem's complexity and the characteristics of the dataset. Polynomial kernels are effective for capturing polynomial relationships but may overfit with higher degrees.\n",
    "\n",
    "# 9. **Generalization**: Kernel functions, including polynomial kernels, enhance SVMs' ability to generalize to unseen data by capturing intricate patterns in the dataset while controlling model complexity through parameter tuning.\n",
    "\n",
    "# 10. **Application**: Polynomial kernels are commonly used in SVMs for tasks where nonlinear relationships are prevalent, such as image recognition, text classification, and biological data analysis, demonstrating their importance in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b99f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM with polynomial kernel: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SVM classifier with polynomial kernel\n",
    "# Here, we use a polynomial kernel of degree 3\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of SVM with polynomial kernel: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abece3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "# In Support Vector Regression (SVR), epsilon (\\( \\epsilon \\)) controls the width of the margin within which no penalty is incurred for errors. Here's how increasing the value of epsilon affects the number of support vectors:\n",
    "\n",
    "# 1. **Wider Margin**: Increasing \\( \\epsilon \\) allows for a wider margin around the predicted values, meaning more training points can fall within this margin without penalty.\n",
    "\n",
    "# 2. **More Support Vectors**: As \\( \\epsilon \\) increases, more training points may be considered support vectors because they influence the size of the margin or lie within the margin itself.\n",
    "\n",
    "# 3. **Complexity and Generalization**: Larger \\( \\epsilon \\) values can lead to a more complex model with potentially more support vectors, influencing both the model's flexibility and its ability to generalize to new data.\n",
    "\n",
    "# 4. **Trade-off**: However, increasing \\( \\epsilon \\) excessively might lead to overfitting if the model starts to adapt too closely to individual training instances, reducing its ability to generalize.\n",
    "\n",
    "# 5. **Practical Consideration**: Therefore, the choice of \\( \\epsilon \\) should balance the need for model flexibility with the goal of generalization, typically determined through cross-validation or grid search techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c99ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "# affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "# and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "# The performance of Support Vector Regression (SVR) is significantly influenced by several key parameters: the choice of kernel function, \\( C \\) parameter, \\( \\epsilon \\) parameter, and \\( \\gamma \\) parameter. Let's explore each parameter and its impact:\n",
    "\n",
    "# 1. **Kernel Function**:\n",
    "#    - **Role**: Determines the type of transformation applied to input features to find nonlinear relationships.\n",
    "#    - **Types**: Common kernels include Linear, Polynomial, Radial Basis Function (RBF), and Sigmoid.\n",
    "#    - **Example**: \n",
    "#      - Use **Linear Kernel** for linear relationships.\n",
    "#      - Use **RBF Kernel** for complex, nonlinear relationships (default in many cases).\n",
    "#    - **Impact**: The choice depends on the dataset's characteristics; RBF generally performs well, but others might suit specific data structures better.\n",
    "\n",
    "# 2. **C Parameter**:\n",
    "#    - **Role**: Controls the trade-off between achieving a low training error and minimizing model complexity (regularization parameter).\n",
    "#    - **Higher C**: Allows the model to fit the training data more closely, potentially leading to overfitting.\n",
    "#    - **Lower C**: Emphasizes a larger margin, leading to a simpler model that may underfit the training data.\n",
    "#    - **Example**: \n",
    "#      - Increase **C** when the training data is noise-free and a higher accuracy on training data is desired.\n",
    "#      - Decrease **C** when you suspect noise in the training data or wish to prioritize a simpler model.\n",
    "\n",
    "# 3. **Epsilon Parameter**:\n",
    "#    - **Role**: Specifies the margin of tolerance where no penalty is given to errors. In SVR, it defines a tube around the predicted value within which errors are considered acceptable.\n",
    "#    - **Larger Epsilon**: Allows more data points to be within the margin of tolerance.\n",
    "#    - **Smaller Epsilon**: Tightens the tolerance, potentially increasing the number of support vectors.\n",
    "#    - **Example**: \n",
    "#      - Increase **epsilon** when you expect a higher variance in the data or when you want to allow more flexibility in the predictions.\n",
    "#      - Decrease **epsilon** when you prefer a stricter adherence to the predictions, potentially reducing the number of support vectors.\n",
    "\n",
    "# 4. **Gamma Parameter**:\n",
    "#    - **Role**: Defines how far the influence of a single training example reaches (only for RBF kernel).\n",
    "#    - **Higher Gamma**: Results in a more complex decision boundary, potentially leading to overfitting.\n",
    "#    - **Lower Gamma**: Results in a smoother decision boundary, potentially underfitting the training data.\n",
    "#    - **Example**: \n",
    "#      - Increase **gamma** to make the model fit the training data more closely, especially if the data is non-linear.\n",
    "#      - Decrease **gamma** to prevent overfitting, especially when dealing with noisy data or a large number of features.\n",
    "\n",
    "# **Considerations**:\n",
    "# - **Parameter Tuning**: Optimal values for these parameters are often found through techniques like grid search or cross-validation.\n",
    "# - **Dataset Characteristics**: The impact of these parameters can vary depending on the dataset's size, complexity, and noise levels.\n",
    "# - **Balance**: Balancing these parameters is crucial to achieve a model that generalizes well to unseen data while fitting the training data appropriately.\n",
    "\n",
    "# By understanding and appropriately adjusting these parameters, SVR can effectively model complex relationships in data and produce robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04321dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment:\n",
    "# L Import the necessary libraries and load the dataseg\n",
    "# L Split the dataset into training and testing setZ\n",
    "# L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "# L Create an instance of the SVC classifier and train it on the training datW\n",
    "# L hse the trained classifier to predict the labels of the testing datW\n",
    "# L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "# precision, recall, F1-scoreK\n",
    "# L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "# improve its performanc_\n",
    "# L Train the tuned classifier on the entire dataseg\n",
    "# L Save the trained classifier to a file for future use.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform scaling (example: using MinMaxScaler)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of SVC classifier\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict labels on the testing set\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'svm_classifier.pkl')\n",
    "print(\"Trained classifier saved to svm_classifier.pkl\")\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform scaling (example: using MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of SVC classifier\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict labels on the testing set\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'svm_classifier.pkl')\n",
    "print(\"Trained classifier saved to svm_classifier.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
